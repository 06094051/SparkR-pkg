{"name":"SparkR","tagline":"R frontend for Spark","body":"# R on Spark\r\nSparkR is an R package that provides a light-weight frontend to use Apache Spark from R. SparkR exposes the Spark API through the `RDD` class and allows users to interactively run jobs from the R shell on a cluster.\r\n\r\n## Features\r\n\r\n# RDDs as Distributed Lists\r\nSparkR exposes the RDD API of Spark as distributed lists in R. For example we can read an input file from HDFS and process every line using `lapply` on a RDD.\r\n\r\n      sc <- sparkR.init(\"local\")\r\n      lines <- textFile(sc, \"hdfs://data.txt\")\r\n      wordsPerLine <- lapply(lines, function(line) { length(unlist(strsplit(line, \" \"))) })\r\n\r\nIn addition to `lapply`, SparkR also allows closures to be applied on every partition using `lapplyWithPartition`. Other supported RDD functions include operations like `reduce`, `reduceByKey`, `groupByKey` and `collect`.\r\n\r\n# Serializing closures\r\nSparkR automatically serializes the necessary variables to execute a function on the cluster. For example if you use some global variables in a function passed to `lapply`, SparkR will automatically capture these variables and copy them to the cluster. An example of using a random weight vector to initialize a matrix is shown below\r\n\r\n       lines <- textFile(sc, \"hdfs://data.txt\")\r\n       initialWeights <- runif(n=D, min = -1, max = 1)\r\n       createMatrix <- function(line) {\r\n         as.numeric(unlist(strsplit(line, \" \"))) %*% t(initialWeights)\r\n       }\r\n       # initialWeights is automatically serialized\r\n       matrixRDD <- lapply(lines, createMatrix)\r\n\r\n# Using existing R packages\r\nSparkR also allows easy use of existing R packages inside closures. The `includePackage` command can be used to indicate packages that should be loaded before every closure is executed on the cluster. For example to use the `Matrix` in a closure applied on each partition of an RDD, you could run\r\n\r\n      generateSparse <- function(x) {\r\n        # Use sparseMatrix function from the Matrix package\r\n        sparseMatrix(i=c(1, 2, 3), j=c(1, 2, 3), x=c(1, 2, 3))\r\n      }\r\n      includePackage(sc, Matrix)\r\n      sparseMat <- lapplyPartition(rdd, generateSparse)\r\n\r\n## Installing SparkR\r\nSparkR requires Scala 2.10 and Spark version >= 0.9.0 and depends on R packages `rJava` and `testthat` (only required for running unit tests). \r\n\r\nIf you wish to try out SparkR, you can use `install_git` from the `devtools` package to directly install the package.\r\n\r\n    library(devtools)\r\n    install_git(\"https://github.com/amplab-extras/SparkR-pkg.git\",\r\n                subdir=\"pkg\")\r\n\r\nIf you wish to clone the repository and build from source, you can using the following script to build the package locally.\r\n\r\n    ./install-dev.sh\r\n\r\n## Running sparkR\r\nIf you have installed it directly from github, you can include the SparkR package and then initialize a SparkContext. For example to run with a local Spark master you can launch R and then run\r\n\r\n    library(SparkR)\r\n    sc <- sparkR.init(master=\"local\")\r\n\r\nIf you have cloned and built SparkR, you can start using it by launching the SparkR shell with\r\n\r\n    ./sparkR\r\n\r\nSparkR also comes with several sample programs in the `examples` directory.\r\nTo run one of them, use `./sparkR <filename> <args>`. For example:\r\n\r\n    ./sparkR examples/pi.R local[2]  \r\n\r\nYou can also run the unit-tests for SparkR by running\r\n\r\n    ./run-tests.sh","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}