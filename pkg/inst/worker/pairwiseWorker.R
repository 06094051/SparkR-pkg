# Worker class
# FIXME: refactor me and worker.R to reduce code duplication

source_local <- function(fname) {
  argv <- commandArgs(trailingOnly = FALSE)
  base_dir <- dirname(substring(argv[grep("--file=", argv)], 8))
  source(paste(base_dir, fname, sep="/"))
}

source_local("serialize.R")

# Set libPaths to include SparkR package as loadNamespace needs this
# TODO: Figure out if we can avoid this by not loading any objects that require
# SparkR namespace
sparkHome <- Sys.getenv("SPARK_HOME")
.libPaths(c( .libPaths(), paste(sparkHome,"/R/lib", sep="")))

# NOTE: We use "stdin" to get the process stdin instead of the command line
inputCon  <- file("stdin", open = "rb")
#outputFileName <- tempfile(pattern="spark-exec", fileext=".out")

outputFileName <- readLines(inputCon, n = 1)
outputCon <- file(outputFileName, open="wb")

# First read the hash function
hashFunction <- unserialize(readRaw(inputCon))

isSerialized <- readInt(inputCon)

# Redirect stdout to stderr to prevent print statements from
# interfering with outputStream
sink(stderr())

if (isSerialized) {
  # Now read as many characters as described in funcLen
  data <- unserialize(readRaw(inputCon))
} else {
  data <- readLines(inputCon)
}


#sink(stderr())
#print(execFunction)
#sink()

output <- hashFunction(data) # FIXME

writeRaw(outputCon, output)
writeInt(outputCon, 0L)

close(outputCon)

# Restore stdout
sink()

# Finally print the name of the output file
cat(outputFileName, "\n")
